{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e35c14f",
      "metadata": {
        "id": "5e35c14f",
        "outputId": "3f52265e-7d18-417d-9ac7-41f6d055bcb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LOADING ALL DATA FILES\n",
            "================================================================================\n",
            "\n",
            "2. Loading CSV files...\n",
            "   ✓ Loaded: dataset.csv (4920 rows)\n",
            "   ✓ Loaded: symptom_Description.csv (41 rows)\n",
            "   ✓ Loaded: symptom_precaution.csv (41 rows)\n",
            "   ✓ Loaded: Symptom-severity.csv (133 rows)\n",
            "\n",
            "3. Loading XML files from MedQuAD...\n",
            "   Found 9 subdirectories\n",
            "   Processing 1_CancerGov_QA: 116 files\n",
            "   Processing 2_GARD_QA: 2685 files\n",
            "   Processing 3_GHR_QA: 1086 files\n",
            "   Processing 4_MPlus_Health_Topics_QA: 981 files\n",
            "   Processing 5_NIDDK_QA: 157 files\n",
            "   Processing 6_NINDS_QA: 277 files\n",
            "   Processing 7_SeniorHealth_QA: 48 files\n",
            "   Processing 8_NHLBI_QA_XML: 88 files\n",
            "   Processing 9_CDC_QA: 59 files\n",
            "   ✓ 1_CancerGov_QA: 729 Q&A pairs\n",
            "   ✓ 2_GARD_QA: 5394 Q&A pairs\n",
            "   ✓ 3_GHR_QA: 5430 Q&A pairs\n",
            "   ✓ 4_MPlus_Health_Topics_QA: 981 Q&A pairs\n",
            "   ✓ 5_NIDDK_QA: 1192 Q&A pairs\n",
            "   ✓ 6_NINDS_QA: 1088 Q&A pairs\n",
            "   ✓ 7_SeniorHealth_QA: 769 Q&A pairs\n",
            "   ✓ 8_NHLBI_QA_XML: 559 Q&A pairs\n",
            "   ✓ 9_CDC_QA: 270 Q&A pairs\n",
            "\n",
            "   Total Q&A pairs extracted: 16412\n",
            "\n",
            "================================================================================\n",
            "LOADING COMPLETE - SUMMARY\n",
            "================================================================================\n",
            "CSV files loaded: 4\n",
            "XML source directories processed: 9\n",
            "\n",
            "================================================================================\n",
            "SAMPLE DATA PREVIEW\n",
            "================================================================================\n",
            "\n",
            "--- CSV: Dataset (first 3 rows) ---\n",
            "            Disease   Symptom_1              Symptom_2              Symptom_3  \\\n",
            "0  Fungal infection     itching              skin_rash   nodal_skin_eruptions   \n",
            "1  Fungal infection   skin_rash   nodal_skin_eruptions    dischromic _patches   \n",
            "2  Fungal infection     itching   nodal_skin_eruptions    dischromic _patches   \n",
            "\n",
            "              Symptom_4 Symptom_5 Symptom_6 Symptom_7 Symptom_8 Symptom_9  \\\n",
            "0   dischromic _patches       NaN       NaN       NaN       NaN       NaN   \n",
            "1                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "2                   NaN       NaN       NaN       NaN       NaN       NaN   \n",
            "\n",
            "  Symptom_10 Symptom_11 Symptom_12 Symptom_13 Symptom_14 Symptom_15  \\\n",
            "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "\n",
            "  Symptom_16 Symptom_17  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "\n",
            "--- XML: 1_CancerGov_QA (first 2 Q&A pairs) ---\n",
            "                                focus  \\\n",
            "0  Adult Acute Lymphoblastic Leukemia   \n",
            "1  Adult Acute Lymphoblastic Leukemia   \n",
            "\n",
            "                                            question  \\\n",
            "0  What is (are) Adult Acute Lymphoblastic Leukem...   \n",
            "1  What are the symptoms of Adult Acute Lymphobla...   \n",
            "\n",
            "                                              answer  \n",
            "0  Key Points\\n                    - Adult acute ...  \n",
            "1  Signs and symptoms of adult ALL include fever,...  \n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"LOADING ALL DATA FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 2. Load CSV files from medi_bot directory\n",
        "print(\"\\n2. Loading CSV files...\")\n",
        "csv_files = {\n",
        "    \"dataset\": \"dataset.csv\",\n",
        "    \"symptom_description\": \"symptom_Description.csv\",\n",
        "    \"symptom_precaution\": \"symptom_precaution.csv\",\n",
        "    \"symptom_severity\": \"Symptom-severity.csv\"\n",
        "}\n",
        "\n",
        "csv_frames = {}\n",
        "for name, filename in csv_files.items():\n",
        "    csv_path = pathlib.Path(filename)\n",
        "    if csv_path.exists():\n",
        "        csv_frames[name] = pd.read_csv(csv_path)\n",
        "        print(f\"   ✓ Loaded: {filename} ({len(csv_frames[name])} rows)\")\n",
        "    else:\n",
        "        print(f\"   ⚠ Not found: {filename}\")\n",
        "\n",
        "# 3. Load XML files from MedQuAD directories\n",
        "print(\"\\n3. Loading XML files from MedQuAD...\")\n",
        "medquad_dir = pathlib.Path(\"MedQuAD\")\n",
        "xml_data = defaultdict(list)\n",
        "\n",
        "if medquad_dir.exists():\n",
        "    subdirs = [d for d in medquad_dir.iterdir() if d.is_dir()]\n",
        "    print(f\"   Found {len(subdirs)} subdirectories\")\n",
        "\n",
        "    for subdir in subdirs:\n",
        "        xml_files = list(subdir.glob(\"*.xml\"))\n",
        "        print(f\"   Processing {subdir.name}: {len(xml_files)} files\")\n",
        "\n",
        "        for xml_file in xml_files:\n",
        "            try:\n",
        "                tree = ET.parse(xml_file)\n",
        "                root = tree.getroot()\n",
        "\n",
        "                doc_info = {\n",
        "                    'source_dir': subdir.name,\n",
        "                    'file': xml_file.name,\n",
        "                    'doc_id': root.get('id'),\n",
        "                    'source': root.get('source'),\n",
        "                    'url': root.get('url'),\n",
        "                    'focus': root.find('Focus').text if root.find('Focus') is not None else None\n",
        "                }\n",
        "\n",
        "                # Extract Q&A pairs\n",
        "                qapairs = root.find('QAPairs')\n",
        "                if qapairs is not None:\n",
        "                    for qapair in qapairs.findall('QAPair'):\n",
        "                        question_elem = qapair.find('Question')\n",
        "                        answer_elem = qapair.find('Answer')\n",
        "\n",
        "                        qa_entry = doc_info.copy()\n",
        "                        qa_entry.update({\n",
        "                            'question_id': question_elem.get('qid') if question_elem is not None else None,\n",
        "                            'question_type': question_elem.get('qtype') if question_elem is not None else None,\n",
        "                            'question': question_elem.text if question_elem is not None else None,\n",
        "                            'answer': answer_elem.text if answer_elem is not None else None\n",
        "                        })\n",
        "\n",
        "                        xml_data[subdir.name].append(qa_entry)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠ Error loading {xml_file.name}: {str(e)}\")\n",
        "\n",
        "    # Convert to DataFrames\n",
        "    xml_frames = {}\n",
        "    total_qa_pairs = 0\n",
        "    for source_dir, data in xml_data.items():\n",
        "        if data:\n",
        "            xml_frames[source_dir] = pd.DataFrame(data)\n",
        "            total_qa_pairs += len(data)\n",
        "            print(f\"   ✓ {source_dir}: {len(data)} Q&A pairs\")\n",
        "\n",
        "    print(f\"\\n   Total Q&A pairs extracted: {total_qa_pairs}\")\n",
        "else:\n",
        "    print(\"   ⚠ MedQuAD directory not found\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LOADING COMPLETE - SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"CSV files loaded: {len(csv_frames)}\")\n",
        "print(f\"XML source directories processed: {len(xml_frames)}\")\n",
        "\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAMPLE DATA PREVIEW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if csv_frames:\n",
        "    print(\"\\n--- CSV: Dataset (first 3 rows) ---\")\n",
        "    if 'dataset' in csv_frames:\n",
        "        print(csv_frames['dataset'].head(3))\n",
        "\n",
        "if xml_frames:\n",
        "    first_xml_key = list(xml_frames.keys())[0]\n",
        "    print(f\"\\n--- XML: {first_xml_key} (first 2 Q&A pairs) ---\")\n",
        "    print(xml_frames[first_xml_key][['focus', 'question', 'answer']].head(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "942748af",
      "metadata": {
        "id": "942748af",
        "outputId": "67655d1b-6d2d-4e98-fea4-014f3fb67441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "STEP 1: LOADING ALL DATA FILES\n",
            "================================================================================\n",
            "\n",
            "1. Loading CSV files...\n",
            "   ✓ Loaded: dataset.csv (4920 rows)\n",
            "   ✓ Loaded: symptom_Description.csv (41 rows)\n",
            "   ✓ Loaded: symptom_precaution.csv (41 rows)\n",
            "   ✓ Loaded: Symptom-severity.csv (133 rows)\n",
            "\n",
            "2. Loading XML files from MedQuAD...\n",
            "   Found 9 subdirectories\n",
            "   Processing 1_CancerGov_QA: 116 files\n",
            "   Processing 2_GARD_QA: 2685 files\n",
            "   Processing 3_GHR_QA: 1086 files\n",
            "   Processing 4_MPlus_Health_Topics_QA: 981 files\n",
            "   Processing 5_NIDDK_QA: 157 files\n",
            "   Processing 6_NINDS_QA: 277 files\n",
            "   Processing 7_SeniorHealth_QA: 48 files\n",
            "   Processing 8_NHLBI_QA_XML: 88 files\n",
            "   Processing 9_CDC_QA: 59 files\n",
            "   ✓ 1_CancerGov_QA: 729 Q&A pairs\n",
            "   ✓ 2_GARD_QA: 5394 Q&A pairs\n",
            "   ✓ 3_GHR_QA: 5430 Q&A pairs\n",
            "   ✓ 4_MPlus_Health_Topics_QA: 981 Q&A pairs\n",
            "   ✓ 5_NIDDK_QA: 1192 Q&A pairs\n",
            "   ✓ 6_NINDS_QA: 1088 Q&A pairs\n",
            "   ✓ 7_SeniorHealth_QA: 769 Q&A pairs\n",
            "   ✓ 8_NHLBI_QA_XML: 559 Q&A pairs\n",
            "   ✓ 9_CDC_QA: 270 Q&A pairs\n",
            "\n",
            "   Total Q&A pairs extracted: 16412\n",
            "\n",
            "================================================================================\n",
            "STEP 2: CLEANING AND NORMALIZING DATA\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 3: CONVERTING TO UNIFIED FORMAT\n",
            "================================================================================\n",
            "\n",
            "1. Processing Symptom-Disease dataset...\n",
            "   ✓ Processed 4920 symptom-disease pairs\n",
            "\n",
            "2. Processing MedQuAD Q&A dataset...\n",
            "   ✓ Processed 249 Q&A pairs from 1_CancerGov_QA\n",
            "   ✓ Processed 2470 Q&A pairs from 2_GARD_QA\n",
            "   ✓ Processed 3595 Q&A pairs from 3_GHR_QA\n",
            "   ✓ Processed 694 Q&A pairs from 4_MPlus_Health_Topics_QA\n",
            "   ✓ Processed 567 Q&A pairs from 5_NIDDK_QA\n",
            "   ✓ Processed 855 Q&A pairs from 6_NINDS_QA\n",
            "   ✓ Processed 439 Q&A pairs from 7_SeniorHealth_QA\n",
            "   ✓ Processed 113 Q&A pairs from 8_NHLBI_QA_XML\n",
            "   ✓ Processed 167 Q&A pairs from 9_CDC_QA\n",
            "\n",
            "================================================================================\n",
            "STEP 4: CREATING FINAL DATASET\n",
            "================================================================================\n",
            "\n",
            "Before deduplication: 14069 entries\n",
            "After deduplication: 9375 entries\n",
            "After filtering short entries: 9375 entries\n",
            "\n",
            "✓ Training set: 8437 entries\n",
            "✓ Validation set: 938 entries\n",
            "\n",
            "✓ Saved training data to: train.csv\n",
            "✓ Saved validation data to: val.csv\n",
            "✓ Saved full dataset to: full_dataset.csv\n",
            "\n",
            "================================================================================\n",
            "SAMPLE DATA PREVIEW\n",
            "================================================================================\n",
            "\n",
            "--- Training Samples (first 3) ---\n",
            "\n",
            "[1]\n",
            "INPUT:  User: how many people are affected by spastic paraplegia type 15 ? Bot:...\n",
            "TARGET: spastic paraplegia type 15 is a rare condition, although its exact prevalence is unknown....\n",
            "\n",
            "[2]\n",
            "INPUT:  User: what research (or clinical trials) is being done for kearns-sayre syndrome ? Bot:...\n",
            "TARGET: the ninds supports research on neuromuscular disorders such as kss. the goals of this research are t...\n",
            "\n",
            "[3]\n",
            "INPUT:  User: how to diagnose uncombable hair syndrome ? Bot:...\n",
            "TARGET: how is uncombable hair syndrome diagnosed? a diagnosis of uncombable hair syndrome (uhs) is made by ...\n",
            "\n",
            "================================================================================\n",
            "DATA SOURCE DISTRIBUTION\n",
            "================================================================================\n",
            "source\n",
            "medquad_3_GHR_QA                    3595\n",
            "medquad_2_GARD_QA                   2470\n",
            "medquad_6_NINDS_QA                   855\n",
            "medquad_4_MPlus_Health_Topics_QA     694\n",
            "medquad_5_NIDDK_QA                   558\n",
            "medquad_7_SeniorHealth_QA            439\n",
            "medquad_1_CancerGov_QA               249\n",
            "symptom_disease                      235\n",
            "medquad_9_CDC_QA                     167\n",
            "medquad_8_NHLBI_QA_XML               113\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "✅ PREPROCESSING COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 1: LOADING ALL DATA FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load CSV files from medi_bot directory\n",
        "print(\"\\n1. Loading CSV files...\")\n",
        "csv_files = {\n",
        "    \"dataset\": \"dataset.csv\",\n",
        "    \"symptom_description\": \"symptom_Description.csv\",\n",
        "    \"symptom_precaution\": \"symptom_precaution.csv\",\n",
        "    \"symptom_severity\": \"Symptom-severity.csv\"\n",
        "}\n",
        "\n",
        "csv_frames = {}\n",
        "for name, filename in csv_files.items():\n",
        "    csv_path = pathlib.Path(filename)\n",
        "    if csv_path.exists():\n",
        "        csv_frames[name] = pd.read_csv(csv_path)\n",
        "        print(f\"   ✓ Loaded: {filename} ({len(csv_frames[name])} rows)\")\n",
        "    else:\n",
        "        print(f\"   ⚠ Not found: {filename}\")\n",
        "\n",
        "# Load XML files from MedQuAD directories\n",
        "print(\"\\n2. Loading XML files from MedQuAD...\")\n",
        "medquad_dir = pathlib.Path(\"MedQuAD\")\n",
        "xml_data = defaultdict(list)\n",
        "\n",
        "if medquad_dir.exists():\n",
        "    subdirs = [d for d in medquad_dir.iterdir() if d.is_dir()]\n",
        "    print(f\"   Found {len(subdirs)} subdirectories\")\n",
        "\n",
        "    for subdir in subdirs:\n",
        "        xml_files = list(subdir.glob(\"*.xml\"))\n",
        "        print(f\"   Processing {subdir.name}: {len(xml_files)} files\")\n",
        "\n",
        "        for xml_file in xml_files:\n",
        "            try:\n",
        "                tree = ET.parse(xml_file)\n",
        "                root = tree.getroot()\n",
        "\n",
        "                doc_info = {\n",
        "                    'source_dir': subdir.name,\n",
        "                    'file': xml_file.name,\n",
        "                    'doc_id': root.get('id'),\n",
        "                    'source': root.get('source'),\n",
        "                    'url': root.get('url'),\n",
        "                    'focus': root.find('Focus').text if root.find('Focus') is not None else None\n",
        "                }\n",
        "\n",
        "                # Extract Q&A pairs\n",
        "                qapairs = root.find('QAPairs')\n",
        "                if qapairs is not None:\n",
        "                    for qapair in qapairs.findall('QAPair'):\n",
        "                        question_elem = qapair.find('Question')\n",
        "                        answer_elem = qapair.find('Answer')\n",
        "\n",
        "                        qa_entry = doc_info.copy()\n",
        "                        qa_entry.update({\n",
        "                            'question_id': question_elem.get('qid') if question_elem is not None else None,\n",
        "                            'question_type': question_elem.get('qtype') if question_elem is not None else None,\n",
        "                            'question': question_elem.text if question_elem is not None else None,\n",
        "                            'answer': answer_elem.text if answer_elem is not None else None\n",
        "                        })\n",
        "\n",
        "                        xml_data[subdir.name].append(qa_entry)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   ⚠ Error loading {xml_file.name}: {str(e)}\")\n",
        "\n",
        "    # Convert to DataFrames\n",
        "    xml_frames = {}\n",
        "    total_qa_pairs = 0\n",
        "    for source_dir, data in xml_data.items():\n",
        "        if data:\n",
        "            xml_frames[source_dir] = pd.DataFrame(data)\n",
        "            total_qa_pairs += len(data)\n",
        "            print(f\"   ✓ {source_dir}: {len(data)} Q&A pairs\")\n",
        "\n",
        "    print(f\"\\n   Total Q&A pairs extracted: {total_qa_pairs}\")\n",
        "else:\n",
        "    print(\"   ⚠ MedQuAD directory not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2: CLEANING AND NORMALIZING DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Text cleaning function\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text data\"\"\"\n",
        "    if pd.isna(text) or text is None:\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 3: CONVERTING TO UNIFIED FORMAT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "unified_data = []\n",
        "\n",
        "# 1. Process Symptom-Disease Dataset\n",
        "print(\"\\n1. Processing Symptom-Disease dataset...\")\n",
        "if 'dataset' in csv_frames and 'symptom_description' in csv_frames and 'symptom_precaution' in csv_frames:\n",
        "    dataset_df = csv_frames['dataset']\n",
        "    description_df = csv_frames['symptom_description']\n",
        "    precaution_df = csv_frames['symptom_precaution']\n",
        "\n",
        "    # Merge datasets\n",
        "    disease_info = description_df.set_index('Disease').to_dict('index')\n",
        "    precaution_info = precaution_df.set_index('Disease').to_dict('index')\n",
        "\n",
        "    for idx, row in dataset_df.iterrows():\n",
        "        disease = row['Disease']\n",
        "\n",
        "        # Collect symptoms\n",
        "        symptoms = []\n",
        "        for col in dataset_df.columns:\n",
        "            if col.startswith('Symptom_'):\n",
        "                symptom = row[col]\n",
        "                if pd.notna(symptom) and symptom.strip():\n",
        "                    symptoms.append(clean_text(symptom))\n",
        "\n",
        "        if symptoms and disease:\n",
        "            # Create symptom-based query\n",
        "            symptoms_text = \", \".join(symptoms[:5])  # Limit to 5 symptoms\n",
        "            input_text = f\"User: I have {symptoms_text}. What could this be? Bot:\"\n",
        "\n",
        "            # Get description\n",
        "            description = \"\"\n",
        "            if disease in disease_info and 'Description' in disease_info[disease]:\n",
        "                description = clean_text(disease_info[disease]['Description'])\n",
        "\n",
        "            # Get precautions\n",
        "            precautions = []\n",
        "            if disease in precaution_info:\n",
        "                for i in range(1, 5):\n",
        "                    prec_key = f'Precaution_{i}'\n",
        "                    if prec_key in precaution_info[disease]:\n",
        "                        prec = precaution_info[disease][prec_key]\n",
        "                        if pd.notna(prec) and prec.strip():\n",
        "                            precautions.append(clean_text(prec))\n",
        "\n",
        "            # Build target text\n",
        "            target_text = f\"You may be experiencing {clean_text(disease)}.\"\n",
        "            if description:\n",
        "                target_text += f\" {description}\"\n",
        "            if precautions:\n",
        "                target_text += f\" Recommended precautions: {', '.join(precautions)}.\"\n",
        "\n",
        "            unified_data.append({\n",
        "                'input_text': input_text,\n",
        "                'target_text': clean_text(target_text),\n",
        "                'source': 'symptom_disease',\n",
        "                'disease': disease\n",
        "            })\n",
        "\n",
        "    print(f\"   ✓ Processed {len([d for d in unified_data if d['source'] == 'symptom_disease'])} symptom-disease pairs\")\n",
        "\n",
        "# 2. Process MedQuAD Q&A Dataset\n",
        "print(\"\\n2. Processing MedQuAD Q&A dataset...\")\n",
        "for source_dir, df in xml_frames.items():\n",
        "    count = 0\n",
        "    for idx, row in df.iterrows():\n",
        "        question = row['question']\n",
        "        answer = row['answer']\n",
        "\n",
        "        if pd.notna(question) and pd.notna(answer) and question.strip() and answer.strip():\n",
        "            # Clean and format\n",
        "            clean_question = clean_text(question)\n",
        "            clean_answer = clean_text(answer)\n",
        "\n",
        "            # Skip if too short or too long\n",
        "            if len(clean_answer) < 20 or len(clean_answer) > 1000:\n",
        "                continue\n",
        "\n",
        "            input_text = f\"User: {clean_question} Bot:\"\n",
        "            target_text = clean_answer\n",
        "\n",
        "            unified_data.append({\n",
        "                'input_text': input_text,\n",
        "                'target_text': target_text,\n",
        "                'source': f'medquad_{source_dir}',\n",
        "                'focus': row.get('focus', '')\n",
        "            })\n",
        "            count += 1\n",
        "\n",
        "    print(f\"   ✓ Processed {count} Q&A pairs from {source_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 4: CREATING FINAL DATASET\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Convert to DataFrame\n",
        "full_df = pd.DataFrame(unified_data)\n",
        "\n",
        "# Remove duplicates\n",
        "print(f\"\\nBefore deduplication: {len(full_df)} entries\")\n",
        "full_df = full_df.drop_duplicates(subset=['input_text', 'target_text'])\n",
        "print(f\"After deduplication: {len(full_df)} entries\")\n",
        "\n",
        "# Remove entries with empty text\n",
        "full_df = full_df[\n",
        "    (full_df['input_text'].str.len() > 10) &\n",
        "    (full_df['target_text'].str.len() > 10)\n",
        "]\n",
        "print(f\"After filtering short entries: {len(full_df)} entries\")\n",
        "\n",
        "# Shuffle the data\n",
        "full_df = full_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split into train and validation\n",
        "train_df, val_df = train_test_split(full_df, test_size=0.1, random_state=42)\n",
        "\n",
        "print(f\"\\n✓ Training set: {len(train_df)} entries\")\n",
        "print(f\"✓ Validation set: {len(val_df)} entries\")\n",
        "\n",
        "# Save to CSV\n",
        "train_path = pathlib.Path(\"train.csv\")\n",
        "val_path = pathlib.Path(\"val.csv\")\n",
        "full_path = pathlib.Path(\"full_dataset.csv\")\n",
        "\n",
        "train_df[['input_text', 'target_text']].to_csv(train_path, index=False)\n",
        "val_df[['input_text', 'target_text']].to_csv(val_path, index=False)\n",
        "full_df.to_csv(full_path, index=False)\n",
        "\n",
        "print(f\"\\n✓ Saved training data to: {train_path}\")\n",
        "print(f\"✓ Saved validation data to: {val_path}\")\n",
        "print(f\"✓ Saved full dataset to: {full_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAMPLE DATA PREVIEW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n--- Training Samples (first 3) ---\")\n",
        "for i in range(min(3, len(train_df))):\n",
        "    print(f\"\\n[{i+1}]\")\n",
        "    print(f\"INPUT:  {train_df.iloc[i]['input_text'][:100]}...\")\n",
        "    print(f\"TARGET: {train_df.iloc[i]['target_text'][:100]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA SOURCE DISTRIBUTION\")\n",
        "print(\"=\" * 80)\n",
        "print(full_df['source'].value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✅ PREPROCESSING COMPLETE!\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "hR6bd-SwJKbW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR6bd-SwJKbW",
        "outputId": "77ab38ba-71c2-470e-c2d7-6072d8c709ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MEDICAL CHATBOT - GPT2 FINE-TUNING\n",
            "================================================================================\n",
            "\n",
            "Loading datasets...\n",
            "✓ Training samples: 8437\n",
            "✓ Validation samples: 938\n",
            "\n",
            "Sample training data:\n",
            "INPUT:  User: how many people are affected by spastic paraplegia type 15 ? Bot:...\n",
            "TARGET: spastic paraplegia type 15 is a rare condition, although its exact prevalence is unknown....\n",
            "\n",
            "Initializing GPT-2 tokenizer and model...\n",
            "✓ Batches per epoch: 2110\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "================================================================================\n",
            "STARTING FINE-TUNING...\n",
            "================================================================================\n",
            "\n",
            "\n",
            "================================================================================\n",
            "EPOCH 1/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 7.7596\n",
            "  Step  100/2110 | Loss: 0.6746\n",
            "  Step  200/2110 | Loss: 0.6078\n",
            "  Step  300/2110 | Loss: 0.6406\n",
            "  Step  400/2110 | Loss: 0.7837\n",
            "  Step  500/2110 | Loss: 0.8707\n",
            "  Step  600/2110 | Loss: 0.2789\n",
            "  Step  700/2110 | Loss: 0.5987\n",
            "  Step  800/2110 | Loss: 0.4265\n",
            "  Step  900/2110 | Loss: 0.5548\n",
            "  Step 1000/2110 | Loss: 0.6159\n",
            "  Step 1100/2110 | Loss: 0.6946\n",
            "  Step 1200/2110 | Loss: 0.5218\n",
            "  Step 1300/2110 | Loss: 0.4272\n",
            "  Step 1400/2110 | Loss: 0.8011\n",
            "  Step 1500/2110 | Loss: 0.5628\n",
            "  Step 1600/2110 | Loss: 0.6015\n",
            "  Step 1700/2110 | Loss: 0.5101\n",
            "  Step 1800/2110 | Loss: 0.5619\n",
            "  Step 1900/2110 | Loss: 0.4098\n",
            "  Step 2000/2110 | Loss: 0.5284\n",
            "  Step 2100/2110 | Loss: 0.3907\n",
            "\n",
            "  Epoch 1 Summary:\n",
            "  ├─ Average Train Loss: 0.5518\n",
            "  ├─ Validation Loss: 0.4664\n",
            "  └─ Validation Perplexity: 1.59\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch1\n",
            "\n",
            "================================================================================\n",
            "EPOCH 2/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.7132\n",
            "  Step  100/2110 | Loss: 0.5168\n",
            "  Step  200/2110 | Loss: 0.6947\n",
            "  Step  300/2110 | Loss: 0.2991\n",
            "  Step  400/2110 | Loss: 0.3577\n",
            "  Step  500/2110 | Loss: 0.7573\n",
            "  Step  600/2110 | Loss: 0.5443\n",
            "  Step  700/2110 | Loss: 0.2192\n",
            "  Step  800/2110 | Loss: 0.2985\n",
            "  Step  900/2110 | Loss: 0.5093\n",
            "  Step 1000/2110 | Loss: 0.4295\n",
            "  Step 1100/2110 | Loss: 0.8129\n",
            "  Step 1200/2110 | Loss: 0.4915\n",
            "  Step 1300/2110 | Loss: 0.3521\n",
            "  Step 1400/2110 | Loss: 0.3349\n",
            "  Step 1500/2110 | Loss: 0.5786\n",
            "  Step 1600/2110 | Loss: 0.3661\n",
            "  Step 1700/2110 | Loss: 0.3001\n",
            "  Step 1800/2110 | Loss: 0.3846\n",
            "  Step 1900/2110 | Loss: 0.4853\n",
            "  Step 2000/2110 | Loss: 0.5063\n",
            "  Step 2100/2110 | Loss: 0.4544\n",
            "\n",
            "  Epoch 2 Summary:\n",
            "  ├─ Average Train Loss: 0.4553\n",
            "  ├─ Validation Loss: 0.4366\n",
            "  └─ Validation Perplexity: 1.55\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch2\n",
            "\n",
            "================================================================================\n",
            "EPOCH 3/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.5187\n",
            "  Step  100/2110 | Loss: 0.7412\n",
            "  Step  200/2110 | Loss: 0.5499\n",
            "  Step  300/2110 | Loss: 0.4793\n",
            "  Step  400/2110 | Loss: 0.3261\n",
            "  Step  500/2110 | Loss: 0.4862\n",
            "  Step  600/2110 | Loss: 0.1806\n",
            "  Step  700/2110 | Loss: 0.5137\n",
            "  Step  800/2110 | Loss: 0.3288\n",
            "  Step  900/2110 | Loss: 0.5030\n",
            "  Step 1000/2110 | Loss: 0.1062\n",
            "  Step 1100/2110 | Loss: 0.8437\n",
            "  Step 1200/2110 | Loss: 0.4285\n",
            "  Step 1300/2110 | Loss: 0.4701\n",
            "  Step 1400/2110 | Loss: 0.4166\n",
            "  Step 1500/2110 | Loss: 0.4475\n",
            "  Step 1600/2110 | Loss: 0.4604\n",
            "  Step 1700/2110 | Loss: 0.4695\n",
            "  Step 1800/2110 | Loss: 0.2923\n",
            "  Step 1900/2110 | Loss: 0.3135\n",
            "  Step 2000/2110 | Loss: 0.7399\n",
            "  Step 2100/2110 | Loss: 0.4747\n",
            "\n",
            "  Epoch 3 Summary:\n",
            "  ├─ Average Train Loss: 0.4162\n",
            "  ├─ Validation Loss: 0.4196\n",
            "  └─ Validation Perplexity: 1.52\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch3\n",
            "\n",
            "================================================================================\n",
            "EPOCH 4/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.4081\n",
            "  Step  100/2110 | Loss: 0.5950\n",
            "  Step  200/2110 | Loss: 0.5254\n",
            "  Step  300/2110 | Loss: 0.4378\n",
            "  Step  400/2110 | Loss: 0.6210\n",
            "  Step  500/2110 | Loss: 0.4060\n",
            "  Step  600/2110 | Loss: 0.1994\n",
            "  Step  700/2110 | Loss: 0.3788\n",
            "  Step  800/2110 | Loss: 0.2742\n",
            "  Step  900/2110 | Loss: 0.2384\n",
            "  Step 1000/2110 | Loss: 0.5158\n",
            "  Step 1100/2110 | Loss: 0.3681\n",
            "  Step 1200/2110 | Loss: 0.4734\n",
            "  Step 1300/2110 | Loss: 0.6966\n",
            "  Step 1400/2110 | Loss: 0.2483\n",
            "  Step 1500/2110 | Loss: 0.1826\n",
            "  Step 1600/2110 | Loss: 0.5213\n",
            "  Step 1700/2110 | Loss: 0.4141\n",
            "  Step 1800/2110 | Loss: 0.4620\n",
            "  Step 1900/2110 | Loss: 0.2903\n",
            "  Step 2000/2110 | Loss: 0.5407\n",
            "  Step 2100/2110 | Loss: 0.2534\n",
            "\n",
            "  Epoch 4 Summary:\n",
            "  ├─ Average Train Loss: 0.3865\n",
            "  ├─ Validation Loss: 0.4088\n",
            "  └─ Validation Perplexity: 1.51\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch4\n",
            "\n",
            "================================================================================\n",
            "EPOCH 5/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.3719\n",
            "  Step  100/2110 | Loss: 0.3544\n",
            "  Step  200/2110 | Loss: 0.3189\n",
            "  Step  300/2110 | Loss: 0.2426\n",
            "  Step  400/2110 | Loss: 0.4273\n",
            "  Step  500/2110 | Loss: 0.2948\n",
            "  Step  600/2110 | Loss: 0.3749\n",
            "  Step  700/2110 | Loss: 0.6041\n",
            "  Step  800/2110 | Loss: 0.7241\n",
            "  Step  900/2110 | Loss: 0.3921\n",
            "  Step 1000/2110 | Loss: 0.2698\n",
            "  Step 1100/2110 | Loss: 0.3413\n",
            "  Step 1200/2110 | Loss: 0.4588\n",
            "  Step 1300/2110 | Loss: 0.2943\n",
            "  Step 1400/2110 | Loss: 0.2900\n",
            "  Step 1500/2110 | Loss: 0.3145\n",
            "  Step 1600/2110 | Loss: 0.6698\n",
            "  Step 1700/2110 | Loss: 0.3436\n",
            "  Step 1800/2110 | Loss: 0.1955\n",
            "  Step 1900/2110 | Loss: 0.3722\n",
            "  Step 2000/2110 | Loss: 0.5279\n",
            "  Step 2100/2110 | Loss: 0.4060\n",
            "\n",
            "  Epoch 5 Summary:\n",
            "  ├─ Average Train Loss: 0.3621\n",
            "  ├─ Validation Loss: 0.4015\n",
            "  └─ Validation Perplexity: 1.49\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch5\n",
            "\n",
            "================================================================================\n",
            "EPOCH 6/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.4487\n",
            "  Step  100/2110 | Loss: 0.5459\n",
            "  Step  200/2110 | Loss: 0.2289\n",
            "  Step  300/2110 | Loss: 0.1561\n",
            "  Step  400/2110 | Loss: 0.3690\n",
            "  Step  500/2110 | Loss: 0.3228\n",
            "  Step  600/2110 | Loss: 0.5050\n",
            "  Step  700/2110 | Loss: 0.3465\n",
            "  Step  800/2110 | Loss: 0.3000\n",
            "  Step  900/2110 | Loss: 0.2975\n",
            "  Step 1000/2110 | Loss: 0.3338\n",
            "  Step 1100/2110 | Loss: 0.0727\n",
            "  Step 1200/2110 | Loss: 0.4607\n",
            "  Step 1300/2110 | Loss: 0.4034\n",
            "  Step 1400/2110 | Loss: 0.4508\n",
            "  Step 1500/2110 | Loss: 0.2286\n",
            "  Step 1600/2110 | Loss: 0.0842\n",
            "  Step 1700/2110 | Loss: 0.3436\n",
            "  Step 1800/2110 | Loss: 0.3742\n",
            "  Step 1900/2110 | Loss: 0.2690\n",
            "  Step 2000/2110 | Loss: 0.1791\n",
            "  Step 2100/2110 | Loss: 0.3243\n",
            "\n",
            "  Epoch 6 Summary:\n",
            "  ├─ Average Train Loss: 0.3415\n",
            "  ├─ Validation Loss: 0.3975\n",
            "  └─ Validation Perplexity: 1.49\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch6\n",
            "\n",
            "================================================================================\n",
            "EPOCH 7/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.2738\n",
            "  Step  100/2110 | Loss: 0.3524\n",
            "  Step  200/2110 | Loss: 0.3267\n",
            "  Step  300/2110 | Loss: 0.3959\n",
            "  Step  400/2110 | Loss: 0.4125\n",
            "  Step  500/2110 | Loss: 0.3309\n",
            "  Step  600/2110 | Loss: 0.2814\n",
            "  Step  700/2110 | Loss: 0.4453\n",
            "  Step  800/2110 | Loss: 0.5024\n",
            "  Step  900/2110 | Loss: 0.3561\n",
            "  Step 1000/2110 | Loss: 0.2601\n",
            "  Step 1100/2110 | Loss: 0.4514\n",
            "  Step 1200/2110 | Loss: 0.3864\n",
            "  Step 1300/2110 | Loss: 0.3213\n",
            "  Step 1400/2110 | Loss: 0.2944\n",
            "  Step 1500/2110 | Loss: 0.1414\n",
            "  Step 1600/2110 | Loss: 0.1436\n",
            "  Step 1700/2110 | Loss: 0.3318\n",
            "  Step 1800/2110 | Loss: 0.3735\n",
            "  Step 1900/2110 | Loss: 0.1165\n",
            "  Step 2000/2110 | Loss: 0.3671\n",
            "  Step 2100/2110 | Loss: 0.5154\n",
            "\n",
            "  Epoch 7 Summary:\n",
            "  ├─ Average Train Loss: 0.3233\n",
            "  ├─ Validation Loss: 0.3988\n",
            "  └─ Validation Perplexity: 1.49\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch7\n",
            "\n",
            "================================================================================\n",
            "EPOCH 8/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.3100\n",
            "  Step  100/2110 | Loss: 0.2794\n",
            "  Step  200/2110 | Loss: 0.2371\n",
            "  Step  300/2110 | Loss: 0.1048\n",
            "  Step  400/2110 | Loss: 0.3192\n",
            "  Step  500/2110 | Loss: 0.3150\n",
            "  Step  600/2110 | Loss: 0.2186\n",
            "  Step  700/2110 | Loss: 0.3630\n",
            "  Step  800/2110 | Loss: 0.2587\n",
            "  Step  900/2110 | Loss: 0.3504\n",
            "  Step 1000/2110 | Loss: 0.3279\n",
            "  Step 1100/2110 | Loss: 0.2494\n",
            "  Step 1200/2110 | Loss: 0.2193\n",
            "  Step 1300/2110 | Loss: 0.2489\n",
            "  Step 1400/2110 | Loss: 0.3552\n",
            "  Step 1500/2110 | Loss: 0.3951\n",
            "  Step 1600/2110 | Loss: 0.1784\n",
            "  Step 1700/2110 | Loss: 0.2789\n",
            "  Step 1800/2110 | Loss: 0.1720\n",
            "  Step 1900/2110 | Loss: 0.2487\n",
            "  Step 2000/2110 | Loss: 0.3741\n",
            "  Step 2100/2110 | Loss: 0.1968\n",
            "\n",
            "  Epoch 8 Summary:\n",
            "  ├─ Average Train Loss: 0.3069\n",
            "  ├─ Validation Loss: 0.3984\n",
            "  └─ Validation Perplexity: 1.49\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch8\n",
            "\n",
            "================================================================================\n",
            "EPOCH 9/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.4040\n",
            "  Step  100/2110 | Loss: 0.2177\n",
            "  Step  200/2110 | Loss: 0.2278\n",
            "  Step  300/2110 | Loss: 0.2398\n",
            "  Step  400/2110 | Loss: 0.1710\n",
            "  Step  500/2110 | Loss: 0.1031\n",
            "  Step  600/2110 | Loss: 0.3750\n",
            "  Step  700/2110 | Loss: 0.4743\n",
            "  Step  800/2110 | Loss: 0.2043\n",
            "  Step  900/2110 | Loss: 0.1793\n",
            "  Step 1000/2110 | Loss: 0.3076\n",
            "  Step 1100/2110 | Loss: 0.4033\n",
            "  Step 1200/2110 | Loss: 0.3583\n",
            "  Step 1300/2110 | Loss: 0.3098\n",
            "  Step 1400/2110 | Loss: 0.3106\n",
            "  Step 1500/2110 | Loss: 0.3271\n",
            "  Step 1600/2110 | Loss: 0.4691\n",
            "  Step 1700/2110 | Loss: 0.4193\n",
            "  Step 1800/2110 | Loss: 0.3113\n",
            "  Step 1900/2110 | Loss: 0.2926\n",
            "  Step 2000/2110 | Loss: 0.2414\n",
            "  Step 2100/2110 | Loss: 0.4153\n",
            "\n",
            "  Epoch 9 Summary:\n",
            "  ├─ Average Train Loss: 0.2917\n",
            "  ├─ Validation Loss: 0.3977\n",
            "  └─ Validation Perplexity: 1.49\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch9\n",
            "\n",
            "================================================================================\n",
            "EPOCH 10/10\n",
            "================================================================================\n",
            "\n",
            "  Step    0/2110 | Loss: 0.2424\n",
            "  Step  100/2110 | Loss: 0.3296\n",
            "  Step  200/2110 | Loss: 0.2818\n",
            "  Step  300/2110 | Loss: 0.3112\n",
            "  Step  400/2110 | Loss: 0.3313\n",
            "  Step  500/2110 | Loss: 0.1495\n",
            "  Step  600/2110 | Loss: 0.3139\n",
            "  Step  700/2110 | Loss: 0.4396\n",
            "  Step  800/2110 | Loss: 0.2299\n",
            "  Step  900/2110 | Loss: 0.2748\n",
            "  Step 1000/2110 | Loss: 0.1647\n",
            "  Step 1100/2110 | Loss: 0.1035\n",
            "  Step 1200/2110 | Loss: 0.2983\n",
            "  Step 1300/2110 | Loss: 0.2055\n",
            "  Step 1400/2110 | Loss: 0.3850\n",
            "  Step 1500/2110 | Loss: 0.3037\n",
            "  Step 1600/2110 | Loss: 0.2119\n",
            "  Step 1700/2110 | Loss: 0.3040\n",
            "  Step 1800/2110 | Loss: 0.3693\n",
            "  Step 1900/2110 | Loss: 0.2093\n",
            "  Step 2000/2110 | Loss: 0.3331\n",
            "  Step 2100/2110 | Loss: 0.2265\n",
            "\n",
            "  Epoch 10 Summary:\n",
            "  ├─ Average Train Loss: 0.2781\n",
            "  ├─ Validation Loss: 0.4024\n",
            "  └─ Validation Perplexity: 1.50\n",
            "  ✓ Checkpoint saved: models/medical-gpt2-epoch10\n",
            "\n",
            "================================================================================\n",
            "SAVING FINAL MODEL...\n",
            "================================================================================\n",
            "\n",
            "✓ Final model saved to: models/medical-gpt2-final\n",
            "✓ Fine-tuning complete!\n",
            "\n",
            "================================================================================\n",
            "TESTING MODEL WITH SAMPLE MEDICAL QUESTIONS\n",
            "================================================================================\n",
            "\n",
            "Question: I have fever and cough. What could this be?\n",
            "Answer: you may be experiencing cough. an infectious disease caused by the varicella-zoster virus (vzv), which is usually spread by air or through contaminated needles or body fluids. varicella-zoster virus (vzv) is one of many viruses that cause an inflammation of the air sacs in your lungs. this swelling is so bad that it interferes with your daily activities, including working and walking. recommended precautions: consult nearest hospital, vaccination, eat healthy, medication.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Question: What are the symptoms of diabetes?\n",
            "Answer: in most people with diabetes, signs and symptoms include: - feeling tired - hunger - easy bruising or bleeding - constipation - lack of fluid - increased or decreased urination - nausea - vomiting - dark-yellow urine - enlarged liver - enlarged spleen - decreased kidney function - enlarged spleen - decreased kidney function - decreased ability to urinate - decreased appetite - weight loss - lightheadedness, irritability, and depression - dry skin - dry mouth - constipation - constipation - constipation with or without cramps - diarrhea - abdominal pain - abdominal cramping - nausea - vomiting - shortness of breath - sweating - weakness - dizziness - weakness\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Question: What is hypertension?\n",
            "Answer: hypertension is a disease that causes blood to clot excessively in your arteries. this is usually caused by a blood clot that has formed in your arteries. if you have hypertension, the arteries in your heart do not produce enough blood to clot properly. if this happens, the blood may not be able to reach your body's cells and organs. this can lead to cardiomyopathy, heart failure, and even death.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "================================================================================\n",
            "✅ TRAINING AND TESTING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "📁 Model saved in: models/medical-gpt2-final\n",
            "🎯 Ready for deployment!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import pandas as pd\n",
        "from torch.optim import AdamW\n",
        "import pathlib\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MEDICAL CHATBOT - GPT2 FINE-TUNING\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Load the preprocessed datasets\n",
        "print(\"Loading datasets...\")\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "val_df = pd.read_csv(\"val.csv\")\n",
        "\n",
        "print(f\"✓ Training samples: {len(train_df)}\")\n",
        "print(f\"✓ Validation samples: {len(val_df)}\\n\")\n",
        "\n",
        "# Show sample\n",
        "print(\"Sample training data:\")\n",
        "print(f\"INPUT:  {train_df.iloc[0]['input_text'][:100]}...\")\n",
        "print(f\"TARGET: {train_df.iloc[0]['target_text'][:100]}...\\n\")\n",
        "\n",
        "# PyTorch Dataset class\n",
        "class MedicalQADataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
        "        self.texts = []\n",
        "        for _, row in dataframe.iterrows():\n",
        "            # Combine input and target for training\n",
        "            combined = f\"{row['input_text']} {row['target_text']}\"\n",
        "            self.texts.append(combined)\n",
        "\n",
        "        self.encodings = tokenizer(\n",
        "            self.texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.encodings.input_ids[idx],\n",
        "            'attention_mask': self.encodings.attention_mask[idx],\n",
        "            'labels': self.encodings.input_ids[idx]\n",
        "        }\n",
        "\n",
        "# Setup tokenizer\n",
        "print(\"Initializing GPT-2 tokenizer and model...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = MedicalQADataset(train_df, tokenizer)\n",
        "val_dataset = MedicalQADataset(val_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "\n",
        "print(f\"✓ Batches per epoch: {len(train_loader)}\\n\")\n",
        "\n",
        "# Model setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING FINE-TUNING...\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Create models directory\n",
        "pathlib.Path(\"models\").mkdir(exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EPOCH {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"  Step {step:4d}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            val_losses.append(outputs.loss.item())\n",
        "\n",
        "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "    val_perplexity = torch.exp(torch.tensor(avg_val_loss))\n",
        "\n",
        "    print(f\"\\n  Epoch {epoch + 1} Summary:\")\n",
        "    print(f\"  ├─ Average Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  ├─ Validation Loss: {avg_val_loss:.4f}\")\n",
        "    print(f\"  └─ Validation Perplexity: {val_perplexity:.2f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint_path = f\"models/medical-gpt2-epoch{epoch+1}\"\n",
        "    model.save_pretrained(checkpoint_path)\n",
        "    tokenizer.save_pretrained(checkpoint_path)\n",
        "    print(f\"  ✓ Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"SAVING FINAL MODEL...\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "final_model_path = \"models/medical-gpt2-final\"\n",
        "model.save_pretrained(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "print(f\"✓ Final model saved to: {final_model_path}\")\n",
        "print(\"✓ Fine-tuning complete!\\n\")\n",
        "\n",
        "# Test the model\n",
        "print(\"=\"*80)\n",
        "print(\"TESTING MODEL WITH SAMPLE MEDICAL QUESTIONS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "model.eval()\n",
        "test_questions = [\n",
        "    \"I have fever and cough. What could this be?\",\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"What is hypertension?\",\n",
        "]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for question in test_questions:\n",
        "        prompt = f\"User: {question} Bot:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract answer after \"Bot:\"\n",
        "        if \"Bot:\" in generated:\n",
        "            answer = generated.split(\"Bot:\", 1)[-1].strip()\n",
        "        else:\n",
        "            answer = generated\n",
        "\n",
        "        print(f\"Question: {question}\")\n",
        "        print(f\"Answer: {answer}\\n\")\n",
        "        print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"✅ TRAINING AND TESTING COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n📁 Model saved in: {final_model_path}\")\n",
        "print(f\"🎯 Ready for deployment!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "76429681",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "HUGGING FACE LOGIN\n",
            "============================================================\n",
            "\n",
            "Please login to Hugging Face:\n",
            "1. Go to https://huggingface.co and create an account if you don't have one\n",
            "2. Go to https://huggingface.co/settings/tokens\n",
            "3. Create a new token (Write access)\n",
            "4. Paste it below\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa5571ebd74a47afb0ed7f024b4a7d42",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Login to Hugging Face (you'll need to create an account first at https://huggingface.co)\n",
        "print(\"=\"*60)\n",
        "print(\"HUGGING FACE LOGIN\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "print(\"Please login to Hugging Face:\")\n",
        "print(\"1. Go to https://huggingface.co and create an account if you don't have one\")\n",
        "print(\"2. Go to https://huggingface.co/settings/tokens\")\n",
        "print(\"3. Create a new token (Write access)\")\n",
        "print(\"4. Paste it below\\n\")\n",
        "\n",
        "notebook_login()   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a3a59761",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "UPLOADING FILES INDIVIDUALLY\n",
            "============================================================\n",
            "\n",
            "Uploading files one by one...\n",
            "\n",
            "Uploading config.json... ✓\n",
            "Uploading model.safetensors... "
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c123f2dccea47d384b8b72be006bfa9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'(MaxRetryError(\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/53/f4/53f442902cdb8ab2d0aec6330c1088930be3bff994a004bb1ced28e3169720af/c79c3f36213942b24eb6056c79803446fdbebe8572b57dccea135d49db47caa6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20251015%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251015T103412Z&X-Amz-Expires=86400&X-Amz-Signature=d2df209ab4d54f4b3aff381370228f846c495a1400fb11612491fd4a73528cb5&X-Amz-SignedHeaders=host&partNumber=1&uploadId=vVhZG7qbV_F98_LmLN1PXWjyjiuCh23MyP.pSPpZHCAyP_gQDyVfcukOsTVltGiJrmr2NRtDKBDeuWWfNKI2ZeWebna.l_itbC50tHWAbfablxtFYFKqt3rMFfcl4_MH&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2406)')))\"), '(Request ID: 7ced9562-bba1-4867-86ff-5d50a6fae8f3)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/53/f4/53f442902cdb8ab2d0aec6330c1088930be3bff994a004bb1ced28e3169720af/c79c3f36213942b24eb6056c79803446fdbebe8572b57dccea135d49db47caa6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20251015%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251015T103412Z&X-Amz-Expires=86400&X-Amz-Signature=d2df209ab4d54f4b3aff381370228f846c495a1400fb11612491fd4a73528cb5&X-Amz-SignedHeaders=host&partNumber=1&uploadId=vVhZG7qbV_F98_LmLN1PXWjyjiuCh23MyP.pSPpZHCAyP_gQDyVfcukOsTVltGiJrmr2NRtDKBDeuWWfNKI2ZeWebna.l_itbC50tHWAbfablxtFYFKqt3rMFfcl4_MH&x-id=UploadPart\n",
            "Retrying in 1s [Retry 1/5].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓\n",
            "Uploading generation_config.json... ✓\n",
            "Uploading tokenizer_config.json... ✓\n",
            "⚠ tokenizer.json not found\n",
            "Uploading vocab.json... ✓\n",
            "Uploading merges.txt... ✓\n",
            "\n",
            "============================================================\n",
            "✅ UPLOAD COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Check your model: https://huggingface.co/Branis333/symptom-gpt2-chatbot\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"UPLOADING FILES INDIVIDUALLY\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "USERNAME = \"Branis333\"\n",
        "MODEL_NAME = \"symptom-gpt2-chatbot\"\n",
        "repo_id = f\"{USERNAME}/{MODEL_NAME}\"\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Create repo\n",
        "try:\n",
        "    create_repo(repo_id=repo_id, private=False, exist_ok=True)\n",
        "    print(f\"✓ Repository ready\\n\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Files to upload\n",
        "model_dir = Path(\"models/medical-gpt2-final\")\n",
        "files_to_upload = [\n",
        "    \"config.json\",\n",
        "    \"model.safetensors\",\n",
        "    \"generation_config.json\",\n",
        "    \"tokenizer_config.json\",\n",
        "    \"tokenizer.json\",\n",
        "    \"vocab.json\",\n",
        "    \"merges.txt\",\n",
        "]\n",
        "\n",
        "print(\"Uploading files one by one...\\n\")\n",
        "for filename in files_to_upload:\n",
        "    file_path = model_dir / filename\n",
        "    \n",
        "    if file_path.exists():\n",
        "        try:\n",
        "            print(f\"Uploading {filename}...\", end=\" \")\n",
        "            api.upload_file(\n",
        "                path_or_fileobj=str(file_path),\n",
        "                path_in_repo=filename,\n",
        "                repo_id=repo_id,\n",
        "                repo_type=\"model\",\n",
        "            )\n",
        "            print(\"✓\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ {e}\")\n",
        "    else:\n",
        "        print(f\"⚠ {filename} not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ UPLOAD COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nCheck your model: https://huggingface.co/{repo_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6ddb3781",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CREATING AND UPLOADING MODEL CARD\n",
            "============================================================\n",
            "\n",
            "Step 1: Saving README.md locally...\n",
            "✓ Saved to README.md\n",
            "\n",
            "Step 2: Uploading to Hugging Face...\n",
            "  Attempt 1/3... ✓\n",
            "\n",
            "============================================================\n",
            "✅ README CARD UPLOADED SUCCESSFULLY!\n",
            "============================================================\n",
            "\n",
            "View your model page: https://huggingface.co/Branis333/symptom-gpt2-chatbot\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import HfApi\n",
        "import time\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CREATING AND UPLOADING MODEL CARD\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "USERNAME = \"Branis333\"\n",
        "MODEL_NAME = \"symptom-gpt2-chatbot\"\n",
        "repo_id = f\"{USERNAME}/{MODEL_NAME}\"\n",
        "\n",
        "model_card = f\"\"\"---\n",
        "language: en\n",
        "license: mit\n",
        "tags:\n",
        "- medical\n",
        "- healthcare\n",
        "- chatbot\n",
        "- question-answering\n",
        "- symptoms\n",
        "- diseases\n",
        "- gpt2\n",
        "datasets:\n",
        "- custom-medical-qa\n",
        "- medquad\n",
        "- symptom-disease-dataset\n",
        "metrics:\n",
        "- perplexity\n",
        "- loss\n",
        "model-index:\n",
        "- name: {MODEL_NAME}\n",
        "  results:\n",
        "  - task:\n",
        "      type: text-generation\n",
        "      name: Medical Q&A Generation\n",
        "    metrics:\n",
        "    - name: Perplexity\n",
        "      type: perplexity\n",
        "      value: 1.50\n",
        "    - name: Final Validation Loss\n",
        "      type: loss\n",
        "      value: 0.4024\n",
        "---\n",
        "\n",
        "# 🏥 Medical Symptom Chatbot - GPT2 Fine-tuned\n",
        "\n",
        "A specialized GPT-2 model fine-tuned on medical Q&A data to assist with symptom analysis, disease information, and health-related questions.\n",
        "\n",
        "## 🎯 Model Description\n",
        "\n",
        "This model is based on GPT-2 and has been fine-tuned on a comprehensive medical dataset combining:\n",
        "- **Symptom-Disease mappings** with descriptions and precautions\n",
        "- **MedQuAD dataset** with expert medical Q&A pairs\n",
        "- Custom medical knowledge base\n",
        "\n",
        "**⚠️ IMPORTANT DISCLAIMER:** This model is for informational and educational purposes only. Always consult qualified healthcare professionals for medical advice, diagnosis, or treatment.\n",
        "\n",
        "## 📊 Training Details\n",
        "\n",
        "### Dataset Statistics\n",
        "- **Total Training Samples:** 8,437\n",
        "- **Validation Samples:** 938\n",
        "- **Total Dataset Size:** 9,375 medical Q&A pairs\n",
        "\n",
        "### Training Configuration\n",
        "- **Base Model:** GPT-2 (124M parameters)\n",
        "- **Training Epochs:** 10\n",
        "- **Batch Size:** 4\n",
        "- **Learning Rate:** 3e-5\n",
        "- **Optimizer:** AdamW\n",
        "- **Max Sequence Length:** 512 tokens\n",
        "- **Hardware:** NVIDIA GPU (CUDA enabled)\n",
        "- **Training Time:** ~3.5 hours\n",
        "\n",
        "### Performance Metrics\n",
        "\n",
        "| Epoch | Train Loss | Val Loss | Perplexity |\n",
        "|-------|------------|----------|------------|\n",
        "| 1     | 0.5518     | 0.4664   | 1.59       |\n",
        "| 2     | 0.4553     | 0.4366   | 1.55       |\n",
        "| 3     | 0.4162     | 0.4196   | 1.52       |\n",
        "| 4     | 0.3865     | 0.4088   | 1.51       |\n",
        "| 5     | 0.3621     | 0.4015   | 1.49       |\n",
        "| 6     | 0.3415     | 0.3975   | 1.49       |\n",
        "| 7     | 0.3233     | 0.3988   | 1.49       |\n",
        "| 8     | 0.3069     | 0.3984   | 1.49       |\n",
        "| 9     | 0.2917     | 0.3977   | 1.49       |\n",
        "| **10** | **0.2781** | **0.4024** | **1.50** |\n",
        "\n",
        "**Final Model Performance:**\n",
        "- ✅ Training Loss: **0.2781**\n",
        "- ✅ Validation Loss: **0.4024**\n",
        "- ✅ Validation Perplexity: **1.50**\n",
        "\n",
        "## 🚀 Usage\n",
        "\n",
        "### Basic Usage\n",
        "\n",
        "```python\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"{repo_id}\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Generate response\n",
        "question = \"I have fever and cough. What could this be?\"\n",
        "prompt = f\"User: {{question}} Bot:\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=150,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "answer = response.split(\"Bot:\")[-1].strip()\n",
        "print(answer)\n",
        "```\n",
        "\n",
        "## 💡 Example Queries\n",
        "\n",
        "### Symptom Analysis\n",
        "```\n",
        "User: I have fever and cough. What could this be?\n",
        "Bot: You may be experiencing a respiratory infection...\n",
        "```\n",
        "\n",
        "### Disease Information\n",
        "```\n",
        "User: What are the symptoms of diabetes?\n",
        "Bot: Common symptoms include increased thirst, frequent urination...\n",
        "```\n",
        "\n",
        "## 📁 Dataset Sources\n",
        "\n",
        "1. **Kaggle Symptom-Disease Dataset** - Disease descriptions, symptom mappings, precautions\n",
        "2. **MedQuAD** - Expert-curated medical Q&A from multiple domains\n",
        "\n",
        "## ⚠️ Limitations\n",
        "\n",
        "1. **Not a Medical Professional**: Cannot replace professional medical advice\n",
        "2. **Training Data Bias**: Limited to information in training data\n",
        "3. **Hallucination Risk**: May generate plausible but incorrect information\n",
        "4. **Language**: Primarily English medical texts\n",
        "\n",
        "## 🔒 Ethical Considerations\n",
        "\n",
        "- **Informational Only**: Should not be used for self-diagnosis\n",
        "- **Professional Consultation Required**: Always seek medical professionals for health concerns\n",
        "- **Verification**: Cross-check any medical information with reliable sources\n",
        "\n",
        "## 📄 License\n",
        "\n",
        "MIT License - Free to use with attribution\n",
        "\n",
        "---\n",
        "\n",
        "**Built with ❤️ using Hugging Face Transformers**\n",
        "\n",
        "*Last Updated: October 2024*\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Save locally first\n",
        "print(\"Step 1: Saving README.md locally...\")\n",
        "with open(\"README.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(model_card)\n",
        "print(\"✓ Saved to README.md\\n\")\n",
        "\n",
        "# Step 2: Upload with retry logic\n",
        "print(\"Step 2: Uploading to Hugging Face...\")\n",
        "api = HfApi()\n",
        "\n",
        "max_retries = 3\n",
        "for attempt in range(max_retries):\n",
        "    try:\n",
        "        print(f\"  Attempt {attempt + 1}/{max_retries}...\", end=\" \")\n",
        "        \n",
        "        api.upload_file(\n",
        "            path_or_fileobj=\"README.md\",\n",
        "            path_in_repo=\"README.md\",\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\",\n",
        "        )\n",
        "        \n",
        "        print(\"✓\")\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"✅ README CARD UPLOADED SUCCESSFULLY!\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"\\nView your model page: https://huggingface.co/{repo_id}\")\n",
        "        break\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌\")\n",
        "        print(f\"  Error: {str(e)[:100]}...\")\n",
        "        \n",
        "        if attempt < max_retries - 1:\n",
        "            wait_time = (attempt + 1) * 5\n",
        "            print(f\"  Waiting {wait_time} seconds before retry...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"❌ UPLOAD FAILED AFTER ALL RETRIES\")\n",
        "            print(\"=\"*60)\n",
        "            print(\"\\nManual upload option:\")\n",
        "            print(f\"1. Go to: https://huggingface.co/{repo_id}/tree/main\")\n",
        "            print(f\"2. Click 'Add file' > 'Upload files'\")\n",
        "            print(f\"3. Upload the README.md file from your current directory\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
